{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiva-tech-code/Defect-Detection-in-wall/blob/main/code2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sVmB04EOLxhl",
        "outputId": "03a19716-82e0-424c-b02e-133ceb7ca998"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'drive' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe475be70510>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define paths and categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/dp/Wall_Defects_Dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths and categories\n",
        "DATA_DIR = '/content/drive/My Drive/dp/Wall_Defects_Dataset'\n",
        "OUTPUT_DIR = '/content/drive/My Drive/Augmented_Dataset'\n",
        "CATEGORIES = ['cracks', 'chipping', 'stains', 'paint_flaking', 'holes', 'no_defect']\n",
        "\n",
        "# Create output directory structure\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "for category in CATEGORIES:\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, category), exist_ok=True)\n",
        "\n",
        "# Augmentation pipeline\n",
        "augmentation_pipeline = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.MotionBlur(blur_limit=3, p=0.2),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomScale(scale_limit=0.2, p=0.5),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "])\n",
        "\n",
        "# Preprocessing and augmentation\n",
        "for category in CATEGORIES:\n",
        "    img_dir = os.path.join(DATA_DIR, category)\n",
        "    augmented_img_dir = os.path.join(OUTPUT_DIR, category)\n",
        "    for img_name in tqdm(os.listdir(img_dir), desc=f'Processing {category} images'):\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Failed to load {img_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_name} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Preprocessing\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        enhanced_img = clahe.apply(gray)\n",
        "        resized_img = cv2.resize(enhanced_img, (224, 224))\n",
        "        output_path = os.path.join(augmented_img_dir, f'proc_{img_name}')\n",
        "        cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "        # Augmentation\n",
        "        for i in range(3):\n",
        "            augmented = augmentation_pipeline(image=image)['image']\n",
        "            aug_img_path = os.path.join(augmented_img_dir, f'aug_{i}_{img_name}')\n",
        "            cv2.imwrite(aug_img_path, augmented)\n",
        "\n",
        "# Create CSV Manifest\n",
        "image_paths, labels = [], []\n",
        "for category in CATEGORIES:\n",
        "    category_dir = os.path.join(OUTPUT_DIR, category)\n",
        "    for img_name in os.listdir(category_dir):\n",
        "        img_path = os.path.join(category_dir, img_name)\n",
        "        image_paths.append(img_path)\n",
        "        labels.append(category)\n",
        "df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "manifest_path = os.path.join('/content/drive/My Drive', 'dataset_manifest.csv')\n",
        "df.to_csv(manifest_path, index=False)\n",
        "print(f\"Dataset manifest created as '{manifest_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow opencv-python albumentations pandas matplotlib pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV7RkUEGL2It",
        "outputId": "5bc19a9f-a27c-4a2a-c2fd-3fee1b29c0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XooGkpjAMDvK",
        "outputId": "02deb082-f621-425f-eff9-5c711c73eb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths and categories\n",
        "DATA_DIR = '/content/drive/My Drive/dp/Wall_Defects_Dataset'\n",
        "OUTPUT_DIR = '/content/drive/My Drive/Augmented_Dataset'\n",
        "CATEGORIES = ['cracks', 'chipping', 'stains', 'paint_flaking', 'holes', 'no_defect']\n",
        "\n",
        "# Create output directory structure\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "for category in CATEGORIES:\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, category), exist_ok=True)\n",
        "\n",
        "print(\"Data directories successfully created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SiBsKAGMJqb",
        "outputId": "877e25d3-c6f4-434a-d003-c0a8dd6d152c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Data directories successfully created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Augmentation pipeline\n",
        "augmentation_pipeline = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.MotionBlur(blur_limit=3, p=0.2),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomScale(scale_limit=0.2, p=0.5),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "])\n",
        "\n",
        "# Preprocessing and augmentation\n",
        "for category in CATEGORIES:\n",
        "    img_dir = os.path.join(DATA_DIR, category)\n",
        "    augmented_img_dir = os.path.join(OUTPUT_DIR, category)\n",
        "    for img_name in tqdm(os.listdir(img_dir), desc=f'Processing {category} images'):\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Failed to load {img_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_name} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Preprocessing\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        enhanced_img = clahe.apply(gray)\n",
        "        resized_img = cv2.resize(enhanced_img, (224, 224))\n",
        "        output_path = os.path.join(augmented_img_dir, f'proc_{img_name}')\n",
        "        cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "        # Augmentation\n",
        "        for i in range(3):\n",
        "            augmented = augmentation_pipeline(image=image)['image']\n",
        "            aug_img_path = os.path.join(augmented_img_dir, f'aug_{i}_{img_name}')\n",
        "            cv2.imwrite(aug_img_path, augmented)\n",
        "\n",
        "# Create CSV Manifest\n",
        "image_paths, labels = [], []\n",
        "for category in CATEGORIES:\n",
        "    category_dir = os.path.join(OUTPUT_DIR, category)\n",
        "    for img_name in os.listdir(category_dir):\n",
        "        img_path = os.path.join(category_dir, img_name)\n",
        "        image_paths.append(img_path)\n",
        "        labels.append(category)\n",
        "df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "manifest_path = os.path.join('/content/drive/My Drive', 'dataset_manifest.csv')\n",
        "df.to_csv(manifest_path, index=False)\n",
        "print(f\"Dataset manifest created as '{manifest_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwKIXlvyMOVe",
        "outputId": "7cab6b41-ad12-4752-8fed-e3f6821eecaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing cracks images: 100%|██████████| 45/45 [00:15<00:00,  2.83it/s]\n",
            "Processing chipping images: 100%|██████████| 21/21 [00:02<00:00,  8.93it/s]\n",
            "Processing stains images: 100%|██████████| 49/49 [00:10<00:00,  4.66it/s]\n",
            "Processing paint_flaking images: 100%|██████████| 98/98 [00:15<00:00,  6.38it/s]\n",
            "Processing holes images: 100%|██████████| 50/50 [00:03<00:00, 15.67it/s]\n",
            "Processing no_defect images: 100%|██████████| 50/50 [00:03<00:00, 13.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset manifest created as '/content/drive/My Drive/dataset_manifest.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data Loading\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    OUTPUT_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    OUTPUT_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5Q9ux1FPAun",
        "outputId": "f2d4408e-8ec6-4e89-e4ae-301ae31cb75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1003 images belonging to 6 classes.\n",
            "Found 249 images belonging to 6 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/legacy/preprocessing/image.py:146: UserWarning: Using \".tiff\" files with multiple bands will cause distortion. Please verify your output.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNet\n",
        "\n",
        "def build_mobilenet_model():\n",
        "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    for layer in base_model.layers[:-4]:\n",
        "        layer.trainable = False\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4__FojnyPJGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1KpT8d5uPOvJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}